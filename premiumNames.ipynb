{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/maneth/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain Analysis: {'avg_length': np.float64(6.5), 'common_words': [('bizness', 1), ('energy', 1)], 'common_prefixes': [('bi', 1), ('en', 1)], 'common_suffixes': [('ss', 1), ('gy', 1)]}\n",
      "['bisness', 'biznesz', 'bizness', 'b1zness', 'biznes5', 'energi', 'biznezs', 'energie', '3nergy', 'bizn3ss', 'energy', 'bizne5s', 'energies', 'en3rgy']\n",
      "\n",
      "Generated Premium Domains:\n",
      "bisness: 100\n",
      "biznesz: 100\n",
      "bizness: 100\n",
      "b1zness: 100\n",
      "biznes5: 100\n",
      "energi: 100\n",
      "biznezs: 100\n",
      "energie: 100\n",
      "3nergy: 100\n",
      "bizn3ss: 100\n",
      "energy: 100\n",
      "bizne5s: 100\n",
      "energies: 100\n",
      "en3rgy: 100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import itertools\n",
    "import spacy\n",
    "\n",
    "class PremiumDomainGenerator:\n",
    "    def __init__(self):\n",
    "        # Initialize NLTK\n",
    "        nltk.download('words')\n",
    "        self.english_words = set(words.words())\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")  # Load the spaCy model\n",
    "        \n",
    "        # Common variations for letters/numbers\n",
    "        self.letter_variations = {\n",
    "        \n",
    "            'e': ['3'],\n",
    "            'i': ['1'],\n",
    "            'o': ['0'],\n",
    "            's': ['z', '5'],\n",
    "            'z': ['s'],\n",
    "            'y': ['i', 'ie'],\n",
    "        }\n",
    "        \n",
    "        # Common domain patterns\n",
    "        self.domain_patterns = {\n",
    "            'suffix': ['hub', 'app', 'tech', 'ai', 'io', 'pro', 'lab', 'box', 'space'],\n",
    "            'prefix': ['e', 'i', 'my','our', 'the', 'web', 'smart', 'cyber', 'mage', 'ape'],\n",
    "        }\n",
    "        \n",
    "        # Industry terms\n",
    "        self.industry_terms = {\n",
    "            'tech': ['ai', 'ml', 'crypto', 'web3', 'cloud', 'data', 'cyber', 'tech'],\n",
    "            'business': ['biz', 'corp', 'inc', 'pro', 'group', 'global'],\n",
    "            'social': ['social', 'share', 'connect', 'community', 'network'],\n",
    "            'commerce': ['shop', 'store', 'market', 'buy', 'sell', 'trade'],\n",
    "        }\n",
    "\n",
    "    def analyze_existing_domains(self, domains):\n",
    "        \"\"\"Analyze patterns in existing premium domains\"\"\"\n",
    "        patterns = {\n",
    "            'lengths': [],\n",
    "            'words': [],\n",
    "            'prefixes': [],\n",
    "            'suffixes': []\n",
    "        }\n",
    "        \n",
    "        for domain in domains:\n",
    "            domain = str(domain).lower()\n",
    "            patterns['lengths'].append(len(domain))\n",
    "            \n",
    "            # Extract words\n",
    "            words = re.findall(r'[a-zA-Z]+', domain)\n",
    "            patterns['words'].extend(words)\n",
    "            \n",
    "            # Analyze prefixes and suffixes\n",
    "            if len(words) > 0:\n",
    "                patterns['prefixes'].append(words[0][:2])\n",
    "                patterns['suffixes'].append(words[-1][-2:])\n",
    "        \n",
    "        return {\n",
    "            'avg_length': np.mean(patterns['lengths']),\n",
    "            'common_words': Counter(patterns['words']).most_common(20),\n",
    "            'common_prefixes': Counter(patterns['prefixes']).most_common(10),\n",
    "            'common_suffixes': Counter(patterns['suffixes']).most_common(10)\n",
    "        }\n",
    "\n",
    "    def get_trending_words(self):\n",
    "        \"\"\"Get trending words from various sources\"\"\"\n",
    "        trending_words = set()\n",
    "        \n",
    "        # Add your trending word sources here\n",
    "        # Example: Tech news websites, social media trends, etc.\n",
    "        trending_words.update([\n",
    "            'ai', 'crypto', 'nft', 'defi', 'web3', 'meta', 'blockchain', 'cloud', 'digital', 'smart', 'cyber', 'eco', 'sustainable',\n",
    "    'tech', 'quantum', 'robotics', 'iot', 'vr', 'ar', 'fintech', 'biotech', 'edtech', 'greentech', 'saas', 'drones', 'automation',\n",
    "    '5g', 'neural', 'genomics', 'spacex', 'tesla', 'openai', 'chatgpt', 'startup', 'venture', 'capital', 'equity', 'fund', 'invest',\n",
    "    'trade', 'market', 'economy', 'biz', 'corp', 'inc', 'global', 'holdings', 'enterprise', 'commerce', 'ecommerce', 'retail',\n",
    "    'wholesale', 'supply', 'logistics', 'consulting', 'advisory', 'strategy', 'growth', 'profit', 'revenue', 'sales', 'marketing',\n",
    "    'branding', 'health', 'wellness', 'fitness', 'nutrition', 'mentalhealth', 'wellbeing', 'selfcare', 'mindfulness', 'yoga',\n",
    "    'meditation', 'therapy', 'rehab', 'clinic', 'pharmacy', 'healthcare', 'medical', 'dental', 'optometry', 'chiropractic',\n",
    "    'holistic', 'organic', 'vegan', 'keto', 'paleo', 'supplements', 'vitamins', 'wellnesscoach', 'personaltrainer', 'spa', 'massage',\n",
    "    'travel', 'adventure', 'explore', 'wanderlust', 'foodie', 'gourmet', 'chef', 'recipe', 'fashion', 'style', 'beauty', 'makeup',\n",
    "    'skincare', 'music', 'podcast', 'vlog', 'blog', 'influencer', 'creator', 'artist', 'gamer', 'esports', 'stream', 'movie', 'film',\n",
    "    'cinema', 'theater', 'dance', 'eco', 'green', 'renewable', 'solar', 'wind', 'climate', 'carbon', 'footprint', 'conservation',\n",
    "    'recycling', 'biodiversity', 'permaculture', 'zerowaste', 'plasticfree', 'ethical', 'fairtrade', 'upcycle', 'cleanenergy',\n",
    "    'ecofriendly', 'environment', 'nature', 'wildlife', 'forest', 'ocean', 'planet', 'earth', 'learn', 'study', 'tutor', 'mentor',\n",
    "    'coach', 'academy', 'institute', 'school', 'college', 'university', 'course', 'class', 'workshop', 'seminar', 'webinar',\n",
    "    'training', 'skills', 'knowledge', 'wisdom', 'elearning', 'onlinecourse', 'certification', 'degree', 'diploma', 'curriculum',\n",
    "    'syllabus', 'lesson', 'homework', 'studyguide', 'examprep', 'social', 'network', 'connect', 'community', 'forum', 'group', 'club',\n",
    "    'society', 'meetup', 'event', 'gathering', 'party', 'celebration', 'festival', 'charity', 'nonprofit', 'volunteer', 'support',\n",
    "    'help', 'outreach', 'advocacy', 'campaign', 'movement', 'initiative', 'collaboration', 'partnership', 'team', 'crew', 'squad',\n",
    "    'tribe', 'amazon', 'google', 'facebook', 'apple', 'microsoft', 'netflix', 'spotify', 'uber', 'lyft', 'airbnb', 'tiktok', 'instagram',\n",
    "    'snapchat', 'twitter', 'linkedin', 'pinterest', 'reddit', 'youtube', 'whatsapp', 'wechat', 'telegram', 'slack', 'discord', 'zoom',\n",
    "    'skype', 'shopify', 'salesforce', 'adobe', 'oracle', 'sap', 'intel', 'amd', 'nvidia', 'samsung', 'huawei', 'xiaomi', 'sony',\n",
    "    'lg', 'panasonic', 'nike', 'adidas', 'puma', 'reebok', 'underarmour', 'zara', 'h&m', 'uniqlo', 'ikea', 'starbucks', 'mcdonalds',\n",
    "    'kfc', 'burgerking', 'subway', 'dominos', 'pepsi', 'coca-cola', 'nestle', 'procter', 'gamble', 'johnson', 'johnson', 'pfizer',\n",
    "    'moderna', 'astrazeneca', 'biontech', 'merck', 'novartis', 'roche', 'glaxosmithkline', 'sanofi', 'bayer', 'siemens', 'bosch',\n",
    "    'ge', 'honeywell', 'lockheed', 'martin', 'boeing', 'airbus', 'northrop', 'grumman', 'raytheon', 'general', 'dynamics', 'bae',\n",
    "    'systems', 'thales', 'leonardo', 'rolls-royce', 'mitsubishi', 'hitachi', 'toshiba', 'fujitsu', 'nec', 'canon', 'nikon', 'olympus',\n",
    "    'kodak', 'philips', 'sharp', 'pioneer', 'yamaha', 'suzuki', 'honda', 'toyota', 'nissan', 'mazda', 'subaru', 'mitsubishi', 'hyundai',\n",
    "    'kia', 'daewoo', 'volkswagen', 'bmw', 'mercedes', 'audi', 'porsche', 'ferrari', 'lamborghini', 'maserati', 'bugatti', 'astonmartin',\n",
    "    'bentley', 'rollsroyce', 'jaguar', 'landrover', 'range', 'rover', 'tesla', 'spacex', 'blueorigin', 'virgingalactic', 'nasa',\n",
    "    'esa', 'roscosmos', 'isro', 'jaxa', 'cnes', 'dlr', 'uksa', 'csiro', 'cnsa', 'knes', 'supreme', 'offwhite', 'balenciaga', 'gucci',\n",
    "    'prada', 'louisvuitton', 'chanel', 'dior', 'hermes', 'versace', 'burberry', 'givenchy', 'fendi', 'valentino', 'saintlaurent',\n",
    "    'balmain', 'celine', 'loewe', 'bottegaveneta', 'alexandermcqueen', 'maisonmargiela', 'rickowens', 'commedesgarcons', 'vetements',\n",
    "    'fearofgod', 'yeezy', 'bape', 'stussy', 'supreme', 'palace', 'kith', 'offwhite', 'nike'\n",
    "        ])\n",
    "        \n",
    "        return trending_words\n",
    "\n",
    "    def is_noun(self, word):\n",
    "        \"\"\"Check if the word is a noun using spaCy POS tagging\"\"\"\n",
    "        doc = self.nlp(word)\n",
    "        return doc[0].pos_ == \"NOUN\"  # Check if the word is a noun\n",
    "\n",
    "    def can_be_pluralized_with_s(self, word):\n",
    "        \"\"\"Determine if a word can be pluralized by simply adding 's'.\"\"\"\n",
    "        irregular_plural_exceptions = {'man', 'woman', 'child', 'foot', 'tooth', 'mouse', 'person'}\n",
    "        \n",
    "        if word.lower() in irregular_plural_exceptions:\n",
    "            return False  # Irregular forms, not handled by adding 's'\n",
    "        if word.endswith(('s', 'x', 'z', 'sh', 'ch')):\n",
    "            return False  # Needs 'es'\n",
    "        if word.endswith('y') and len(word) > 1 and word[-2] not in 'aeiou':\n",
    "            return False  # Needs 'ies'\n",
    "        \n",
    "        return True\n",
    "\n",
    "  \n",
    "    def generate_variations(self, word):\n",
    "        \"\"\"Generate variations of a word\"\"\"\n",
    "        variations = set([word])\n",
    "        \n",
    "        # Check if the word is a noun\n",
    "        if self.is_noun(word):\n",
    "            # Plural forms\n",
    "            if self.can_be_pluralized_with_s(word):\n",
    "                variations.add(word + 's')\n",
    "            if word.endswith('y') and len(word) > 1 and word[-2] not in 'aeiou':\n",
    "                variations.add(word[:-1] + 'ies')  # Convert to plural for words like 'baby' -> 'babies'\n",
    "\n",
    "            if word.endswith('ies'):\n",
    "                variations.add(word[:-3] + 'y')  # Handle singular conversion for 'babies' -> 'baby'\n",
    "\n",
    "        for i in range(len(word)):\n",
    "            if word[i] in self.letter_variations:\n",
    "                for variation in self.letter_variations[word[i]]:\n",
    "                    new_word = word[:i] + variation + word[i+1:]\n",
    "                    variations.add(new_word)\n",
    "    \n",
    "        return variations\n",
    "\n",
    "    # def generate_variations(self, word):\n",
    "    #     \"\"\"Generate variations of a word\"\"\"\n",
    "    #     variations = set([word])\n",
    "        \n",
    "    #     # Plural forms\n",
    "\n",
    "    #     variations.add(word + 's')\n",
    "    #     if word.endswith('y'):\n",
    "    #         variations.add(word[:-1] + 'ies')\n",
    "\n",
    "    #     if word.endswith('ies'):\n",
    "    #         variations.add(word[:-3] + 'y')\n",
    "            \n",
    "    #     # Number replacements\n",
    "    #     for i in range(len(word)):\n",
    "    #         if word[i] in self.letter_variations:\n",
    "    #             for variation in self.letter_variations[word[i]]:\n",
    "    #                 new_word = word[:i] + variation + word[i+1:]\n",
    "    #                 variations.add(new_word)\n",
    "        \n",
    "        # Add common suffixes\n",
    "        # for suffix in self.domain_patterns['suffix']:\n",
    "        #     variations.add(word + suffix)\n",
    "            \n",
    "        # # Add common prefixes\n",
    "        # for prefix in self.domain_patterns['prefix']:\n",
    "        #     variations.add(prefix + word)\n",
    "        \n",
    "        # return variations\n",
    "\n",
    "    def generate_premium_domains(self, existing_domains, num_domains=100):\n",
    "        \"\"\"Generate premium domain names\"\"\"\n",
    "        premium_domains = set()\n",
    "        \n",
    "        # Get trending words\n",
    "        trending_words = self.get_trending_words()\n",
    "        \n",
    "        # Combine seed words with trending words\n",
    "        # all_base_words = set(existing_domains) | trending_words\n",
    "        \n",
    "        for domain in existing_domains:\n",
    "            # Generate variations\n",
    "            # print(domain)\n",
    "            variations = self.generate_variations(domain)\n",
    "            \n",
    "            # Combine with industry terms\n",
    "            # for term in itertools.chain(*self.industry_terms.values()):\n",
    "            #     premium_domains.add(f\"{base_word}{term}\")\n",
    "            #     premium_domains.add(f\"{term}{base_word}\")\n",
    "                \n",
    "            # Add variations\n",
    "            premium_domains.update(variations)\n",
    "            \n",
    "            # Create compound domains\n",
    "            # for second_word in all_base_words:\n",
    "            #     if second_word != base_word:\n",
    "            #         premium_domains.add(f\"{base_word}{second_word}\")\n",
    "        \n",
    "        # Filter domains\n",
    "        # print(premium_domains)\n",
    "        filtered_domains = self.filter_domains(premium_domains)\n",
    "        \n",
    "        return list(filtered_domains)\n",
    "\n",
    "    def filter_domains(self, domains):\n",
    "        \"\"\"Filter domains based on quality criteria\"\"\"\n",
    "        filtered = set()\n",
    "        \n",
    "        for domain in domains:\n",
    "            # Length check\n",
    "            if not (2 <= len(domain) <= 20):\n",
    "                continue\n",
    "                \n",
    "            # Character check\n",
    "            if not re.match(r'^[a-zA-Z0-9-]+$', domain):\n",
    "                continue\n",
    "                \n",
    "            # No double hyphens\n",
    "            if '--' in domain:\n",
    "                continue\n",
    "                \n",
    "            # No starting/ending hyphens\n",
    "            if domain.startswith('-') or domain.endswith('-'):\n",
    "                continue\n",
    "            \n",
    "            filtered.add(domain)\n",
    "            \n",
    "        return filtered\n",
    "\n",
    "    def score_domain(self, domain):\n",
    "        \"\"\"Score a domain based on various factors\"\"\"\n",
    "        score = 100\n",
    "        \n",
    "        # Length factor\n",
    "        if len(domain) < 5:\n",
    "            score += 10\n",
    "        elif len(domain) > 10:\n",
    "            score -= 20\n",
    "            \n",
    "        # Contains dictionary word\n",
    "        if any(word in self.english_words for word in re.findall(r'[a-zA-Z]+', domain)):\n",
    "            score += 15\n",
    "            \n",
    "        # Contains trending term\n",
    "        if any(term in domain for term in self.get_trending_words()):\n",
    "            score += 20\n",
    "            \n",
    "        # Contains industry term\n",
    "        if any(term in domain for terms in self.industry_terms.values() for term in terms):\n",
    "            score += 15\n",
    "            \n",
    "        return max(0, min(score, 100))  # Normalize between 0 and 100\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    generator = PremiumDomainGenerator()\n",
    "    \n",
    "    # Load your existing premium domains\n",
    "    df = pd.read_csv('data.csv')\n",
    "    existing_domains = df['domain'].tolist()\n",
    "    existing_domains = ['bizness', 'energy']\n",
    "    \n",
    "    # Analyze existing domains\n",
    "    analysis = generator.analyze_existing_domains(existing_domains)\n",
    "    print(\"Domain Analysis:\", analysis)\n",
    "    \n",
    "    # Generate new domains\n",
    "    seed_words = ['boy', 'tech', 'crypto', 'meta']  # Add your seed words\n",
    "    new_domains = generator.generate_premium_domains(existing_domains)\n",
    "    \n",
    "    print(new_domains)\n",
    "    # Score and sort domains\n",
    "    scored_domains = [(domain, generator.score_domain(domain)) for domain in new_domains]\n",
    "    scored_domains.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nGenerated Premium Domains:\")\n",
    "    for domain, score in scored_domains[:20]:\n",
    "        print(f\"{domain}: {score}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
