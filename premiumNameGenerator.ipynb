{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import random\n",
    "import re\n",
    "\n",
    "class PremiumDomainGenerator:\n",
    "    def __init__(self):\n",
    "        # Download required NLTK data\n",
    "        nltk.download('words')\n",
    "        nltk.download('wordnet')\n",
    "        self.english_words = set(nltk.corpus.words.words())\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 4))\n",
    "        self.classifier = RandomForestClassifier()\n",
    "        \n",
    "    def preprocess_domains(self, domains):\n",
    "        # Convert to string and handle any non-string types\n",
    "        processed_domains = []\n",
    "        for domain in domains:\n",
    "            try:\n",
    "                # Convert to string if not already\n",
    "                domain_str = str(domain)\n",
    "                # Remove TLD if present and convert to lowercase\n",
    "                clean_domain = re.sub(r'\\..*$', '', domain_str.lower())\n",
    "                processed_domains.append(clean_domain)\n",
    "            except (AttributeError, TypeError):\n",
    "                # Skip invalid entries\n",
    "                continue\n",
    "        return processed_domains\n",
    "        \n",
    "    def extract_features(self, domains):\n",
    "        # Extract various features from domains\n",
    "        features = []\n",
    "        for domain in domains:\n",
    "            features.append({\n",
    "                'length': len(domain),\n",
    "                'num_words': len(domain.split()),\n",
    "                'has_numbers': int(bool(re.search(r'\\d', domain))),\n",
    "                'num_vowels': sum(1 for c in domain if c in 'aeiou'),\n",
    "                'num_consonants': sum(1 for c in domain if c in 'bcdfghjklmnpqrstvwxyz'),\n",
    "            })\n",
    "        return pd.DataFrame(features)\n",
    "        \n",
    "    def train(self, domains, labels):\n",
    "        # Preprocess domains\n",
    "        clean_domains = self.preprocess_domains(domains)\n",
    "        \n",
    "        # Ensure we have valid data after preprocessing\n",
    "        if not clean_domains:\n",
    "            raise ValueError(\"No valid domains after preprocessing\")\n",
    "            \n",
    "        # Create character-level features\n",
    "        X_char = self.vectorizer.fit_transform(clean_domains)\n",
    "        \n",
    "        # Create additional features\n",
    "        X_features = self.extract_features(clean_domains)\n",
    "        \n",
    "        # Train the classifier\n",
    "        self.classifier.fit(X_char, labels[:len(clean_domains)])\n",
    "        \n",
    "    def generate_domains(self, num_domains=10, min_length=5, max_length=15):\n",
    "        generated_domains = set()\n",
    "        \n",
    "        while len(generated_domains) < num_domains:\n",
    "            # Generate base domain using character n-grams\n",
    "            length = random.randint(min_length, max_length)\n",
    "            domain = ''\n",
    "            \n",
    "            # Use common patterns from your dataset\n",
    "            patterns = [\n",
    "                'tech', 'hub', 'solutions', 'pay', 'desk', 'pro',\n",
    "                'digital', 'eco', 'cyber', 'smart', 'cloud', 'web'\n",
    "            ]\n",
    "            \n",
    "            if random.random() < 0.3:  # 30% chance to use a pattern\n",
    "                pattern = random.choice(patterns)\n",
    "                remaining_length = length - len(pattern)\n",
    "                if remaining_length > 0:\n",
    "                    prefix = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=remaining_length))\n",
    "                    domain = prefix + pattern\n",
    "            else:\n",
    "                domain = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=length))\n",
    "            \n",
    "            # Evaluate domain quality\n",
    "            if self.evaluate_domain(domain):\n",
    "                generated_domains.add(domain)\n",
    "                \n",
    "        return list(generated_domains)\n",
    "    \n",
    "    def evaluate_domain(self, domain):\n",
    "        # Basic rules for domain quality\n",
    "        if len(domain) < 4:\n",
    "            return False\n",
    "        if domain.startswith('-') or domain.endswith('-'):\n",
    "            return False\n",
    "        if '--' in domain:\n",
    "            return False\n",
    "            \n",
    "        # Check if domain contains recognizable words or patterns\n",
    "        words = [''.join(gram) for gram in ngrams(domain, 3)]\n",
    "        word_like = any(word in self.english_words for word in words)\n",
    "        \n",
    "        # Evaluate using the trained classifier\n",
    "        features = self.vectorizer.transform([domain])\n",
    "        prediction = self.classifier.predict_proba(features)[0]\n",
    "        \n",
    "        # Return True if domain passes all checks and has high prediction score\n",
    "        return word_like and prediction[1] > 0.7\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Load your dataset\n",
    "    df = pd.read_csv('Premium.csv')\n",
    "    \n",
    "    # Convert domains to strings and handle NaN values\n",
    "    domains = df['Name'].astype(str).replace('nan', '').tolist()\n",
    "    labels = df['isPremium'].tolist()\n",
    "    \n",
    "    # Remove empty domains and their corresponding labels\n",
    "    valid_data = [(d, l) for d, l in zip(domains, labels) if d]\n",
    "    if not valid_data:\n",
    "        raise ValueError(\"No valid domains in dataset\")\n",
    "    \n",
    "    valid_domains, valid_labels = zip(*valid_data)\n",
    "    \n",
    "    # Initialize and train the generator\n",
    "    generator = PremiumDomainGenerator()\n",
    "    generator.train(valid_domains, valid_labels)\n",
    "    \n",
    "    # Generate new premium domains\n",
    "    new_domains = generator.generate_domains(num_domains=10)\n",
    "    \n",
    "    print(\"Generated Premium Domains:\")\n",
    "    for domain in new_domains:\n",
    "        print(domain)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
